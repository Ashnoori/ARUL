{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2259801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab710441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c98df584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\noori\\AppData\\Local\\Temp/ipykernel_3644/501681495.py:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f39bef32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7394b4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ef12a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "rul_1_gt = pd.read_csv(\"D:/CMAPSSData/RUL_FD001.txt\", header=None)\n",
    "\n",
    "rul_1_gt.rename(columns={0: \"RUL_1gt\"}, inplace=True)   \n",
    "\n",
    "column_name_dict={ 0: \"engine_id\", 1: \"cycle\", 2: \"op_set_1\", 3: \"op_set_2\", 4: \"op_set_3\", 5:\"sensor_1\", 6: \"sensor_2\",\n",
    "                   7: \"sensor_3\", 8: \"sensor_4\", 9: \"sensor_5\", 10: \"sensor_6\", 11: \"sensor_7\", 12: \"sensor_8\", 13: \"sensor_9\",\n",
    "                  14: \"sensor_10\", 15: \"sensor_11\", 16: \"sensor_12\", 17: \"sensor_13\", 18: \"sensor_14\", 19: \"sensor_15\", 20: \"sensor_16\",\n",
    "                  21: \"sensor_17\", 22: \"sensor_18\", 23: \"sensor_19\", 24: \"sensor_20\", 25: \"sensor_21\", 26: \"sensor_22\", 27: \"sensor_23\"}\n",
    "\n",
    "################   ################   ################   ################ \n",
    "\n",
    "test_1 = pd.read_csv(\"D:/CMAPSSData/test_FD001.txt\", header=None, sep=' ')\n",
    "\n",
    "test_1.rename(columns=column_name_dict, inplace=True)\n",
    "\n",
    "################   ################   ################   ################ \n",
    "\n",
    "train_1 = pd.read_csv(\"D:/CMAPSSData/train_FD001.txt\", header=None, sep=' ')\n",
    "\n",
    "train_1.rename(columns=column_name_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55dd46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feture selection\n",
    "train_1.drop(\n",
    "    columns=[\"op_set_1\",\"op_set_2\",\"op_set_3\",\n",
    "             \"sensor_1\",\"sensor_5\",\"sensor_6\",\n",
    "             \"sensor_10\",\"sensor_16\",\"sensor_18\",\n",
    "             \"sensor_19\",\"sensor_22\",\"sensor_23\"], inplace=True)\n",
    "\n",
    "\n",
    "test_1.drop(\n",
    "    columns=[\"op_set_1\",\"op_set_2\",\"op_set_3\",\n",
    "             \"sensor_1\",\"sensor_5\",\"sensor_6\",\n",
    "             \"sensor_10\",\"sensor_16\",\"sensor_18\",\n",
    "             \"sensor_19\",\"sensor_22\",\"sensor_23\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922549e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute RUL values -- RUL for train_1 \n",
    "rul_1 = pd.DataFrame(train_1.groupby('engine_id')['cycle'].max()).reset_index()\n",
    "rul_1.columns = ['engine_id', 'max']\n",
    "\n",
    "rul_train_1 = train_1.merge(rul_1, on=['engine_id'], how='left')\n",
    "rul_train_1['RUL'] = rul_train_1['max'] - rul_train_1['cycle']\n",
    "rul_train_1.drop(['max'], axis=1, inplace=True)\n",
    "\n",
    "###############################################\n",
    "#Compute RUL values -- RUL for test_1 \n",
    "rul_1_gt[\"engine_id\"]=rul_1_gt.index + 1\n",
    "\n",
    "max_1 = pd.DataFrame(test_1.groupby('engine_id')['cycle'].max()).reset_index()\n",
    "max_1.columns = ['engine_id', 'max']\n",
    "max_test_1 = test_1.merge(max_1, on=['engine_id'], how='left')\n",
    "rul_test_1 = max_test_1.merge(rul_1_gt, on=['engine_id'], how='left')\n",
    "\n",
    "rul_test_1['RUL'] = rul_test_1['max'] - rul_test_1['cycle'] + rul_test_1[\"RUL_1gt\"] \n",
    "rul_test_1.drop(['max', 'RUL_1gt'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f10ff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_1 minmax scaling\n",
    "cols_normalize_1 = rul_train_1.columns.difference(['engine_id','cycle'])\n",
    "\n",
    "scaler_1 = MinMaxScaler()\n",
    "norm_rul_train_1 = pd.DataFrame(scaler_1.fit_transform(rul_train_1[cols_normalize_1]), \n",
    "                                columns=cols_normalize_1, \n",
    "                                index=rul_train_1.index)\n",
    "\n",
    "norm_rul_train_1=pd.concat([norm_rul_train_1, rul_train_1[[\"engine_id\", \"cycle\"]]], axis=1)\n",
    "\n",
    "################   ################   ################   ################   ################\n",
    "\n",
    "#test_1 minmax scaling\n",
    "norm_rul_test_1 = pd.DataFrame(scaler_1.transform(rul_test_1[cols_normalize_1]), \n",
    "                                columns=cols_normalize_1, \n",
    "                                index=rul_test_1.index)\n",
    "\n",
    "norm_rul_test_1=pd.concat([norm_rul_test_1, rul_test_1[[\"engine_id\", \"cycle\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54156e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are only interested in the LAST CYCLE of each engine in the test set as we only have True RUL values for those records.\n",
    "# sort lists by the # of engines\n",
    "g_train_1=norm_rul_train_1.groupby('engine_id')\n",
    "g_test_1=norm_rul_test_1.groupby('engine_id')\n",
    "\n",
    "#list of dfs(engines)\n",
    "train_list = []\n",
    "test_list = []  \n",
    "\n",
    "for engineid in g_train_1.groups.keys():\n",
    "    train_list.append(g_train_1.get_group(engineid)) \n",
    "\n",
    "for engineid in g_test_1.groups.keys():\n",
    "    test_list.append(g_test_1.get_group(engineid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ed9363a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_list), len(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c37c4b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating sequences for each engine --> multivariate one step problem\n",
    "from numpy import array\n",
    "\n",
    "#df: df extracted from train_list, n_steps: window size\n",
    "def split_sequences(df, n_steps):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(df)):\n",
    "        end_ix = i + n_steps\n",
    "        if end_ix > len(df):\n",
    "            break\n",
    "        seq_x, seq_y = df[i:end_ix, 1:], df[end_ix-1, 0]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n",
    "\n",
    "\n",
    "list_x=[]\n",
    "list_y=[]\n",
    "for engine_df in train_list:\n",
    "    #convert df to arr \n",
    "    engine_arr=engine_df.drop(columns=[\"engine_id\", \"cycle\"]).to_numpy()\n",
    "    X, y = split_sequences(engine_arr, 21)#since smallest df has 21 rows\n",
    "    list_x.append(X)\n",
    "    list_y.append(y)\n",
    "\n",
    "#concat alt alta  \n",
    "X_arr=np.concatenate(list_x)\n",
    "y_arr=np.concatenate(list_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15e30720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X_arr, y_arr, test_size=0.1, random_state=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18bd570c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18631, 21, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6d3d4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18631,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b0f1e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16767, 21, 14)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccf304cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16767,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae9ae450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1864, 21, 14)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dbd59cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1864,)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "805b87ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class MultiHeadSelfAttention(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        if embed_dim % num_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
    "            )\n",
    "        self.projection_dim = embed_dim // num_heads\n",
    "        self.query_dense = layers.Dense(embed_dim)\n",
    "        self.key_dense = layers.Dense(embed_dim)\n",
    "        self.value_dense = layers.Dense(embed_dim)\n",
    "        self.combine_heads = layers.Dense(embed_dim)\n",
    "\n",
    "    def attention(self, query, key, value):\n",
    "        score = tf.matmul(query, key, transpose_b=True)\n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "        scaled_score = score / tf.math.sqrt(dim_key)\n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
    "        output = tf.matmul(weights, value)\n",
    "        return output, weights\n",
    "\n",
    "    def separate_heads(self, x, batch_size):\n",
    "        \"\"\"Reshape x to the shape (batch_size, -1, num_heads, embedding dimension.\n",
    "        Used to obtain the separate attention heads in each for each batch.\"\"\"\n",
    "        # -1 to ensure the dimension of the reshaped tensor is compatible with\n",
    "        # the original x\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
    "        # ensure the transposed tensor dimensions are 0, 2, 1, 3\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
    "        query = self.separate_heads(\n",
    "            query, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        key = self.separate_heads(\n",
    "            key, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        value = self.separate_heads(\n",
    "            value, batch_size\n",
    "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
    "        attention, weights = self.attention(query, key, value)\n",
    "        attention = tf.transpose(\n",
    "            attention, perm=[0, 2, 1, 3]\n",
    "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
    "        concat_attention = tf.reshape(\n",
    "            attention, (batch_size, -1, self.embed_dim)\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        # Recombine the heads\n",
    "        output = self.combine_heads(\n",
    "            concat_attention\n",
    "        )  # (batch_size, seq_len, embed_dim)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "520f40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            # note: doesnt use GELU as noted in our text\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Multi-head attention\n",
    "        attn_output = self.att(inputs)\n",
    "        # Apply dropout\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # Add and norm\n",
    "        # NOTE!!! This does not use leaky relu or GELU?\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        # Pass through FFNN, dropout again\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # Final add and norm\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ead084bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlockWithLeakyRelu(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlockWithLeakyRelu, self).__init__()\n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = keras.Sequential(\n",
    "            # note: doesnt use GELU as noted in our text\n",
    "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.leakyrelu = keras.Sequential(\n",
    "            # note: doesnt use GELU as noted in our text\n",
    "            [layers.Dense(ff_dim, activation=\"LeakyReLU\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        # Multi-head attention\n",
    "        attn_output = self.att(inputs)\n",
    "        # Apply dropout\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        # Apply leakyrelu after each sublayer\n",
    "        attn_output = self.leakyrelu(attn_output)\n",
    "        # Add and norm\n",
    "        # NOTE!!! This does not use leaky relu or GELU?\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        # Pass through FFNN, dropout again\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        # Apply leakyrelu\n",
    "        ffn_output = self.leakyrelu(ffn_output)\n",
    "        # Final add and norm\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d021dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    \"\"\"Add positional embedding\"\"\"\n",
    "    def __init__(self, maxlen, embed_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.maxlen = maxlen\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        # maxlen = tf.shape(x)[-1]\n",
    "        maxlen = self.maxlen\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c428af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(layers.Layer):\n",
    "    \"\"\"LinearLayer\"\"\"\n",
    "    def __init__(self, maxlen):\n",
    "        super(LinearLayer, self).__init__()\n",
    "        self.linear = layers.Dense(units=maxlen)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba9d62be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 21, 14), dtype=tf.float32, name='input_5'), name='input_5', description=\"created by layer 'input_5'\")\n",
      "(None, 21, 512)\n",
      "(None, 21, 512)\n"
     ]
    }
   ],
   "source": [
    "# 1. Setting the parameters:\n",
    "\n",
    "seq_len=x_train.shape[1]\n",
    "num_features=x_train.shape[2]\n",
    "\n",
    "embed_dim = 512  # Embedding size for each token\n",
    "num_heads = 4 # 1 - Number of attention heads\n",
    "ff_dim = 512 # 3 - feed forward network hidden dimension * Hidden layer size in feed forward network inside transformer\n",
    "droupout_rate = 0.2  # 4 - Droupoutrate\n",
    "\n",
    "# 2. Constructing the layers:\n",
    "#inputs = layers.Input(shape=(maxlen,))\n",
    "inputs = layers.Input(shape=(seq_len, num_features))\n",
    "print(inputs)\n",
    "#embedding_layer = LinearLayer(maxlen)\n",
    "#embedding_layer = LinearLayer(seq_len)\n",
    "\n",
    "embedding_layer = LinearLayer(embed_dim)\n",
    "positional_embeddings = PositionalEmbedding(maxlen=seq_len, embed_dim=embed_dim)\n",
    "\n",
    "\n",
    "# 3. Building the network:\n",
    "x = embedding_layer(inputs)\n",
    "#print(x.shape)\n",
    "x = positional_embeddings(x) # adding positional embeddings\n",
    "#print(x.shape)\n",
    "#transformer_block = TransformerBlock(maxlen, num_heads, ff_dim)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, droupout_rate)\n",
    "\n",
    "# Try to write a for loop for the number of transfor block\n",
    "# 2 - Number of Transformer_blocks\n",
    "x = transformer_block(x)\n",
    "x = transformer_block(x)\n",
    "x = transformer_block(x)\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(droupout_rate)(x)\n",
    "\n",
    "# 5 - Activation parameter\n",
    "x = layers.Dense(20, activation=\"sigmoid\")(x)\n",
    "\n",
    "x = layers.Dropout(droupout_rate)(x)\n",
    "\n",
    "#outputs = layers.Dense(2, activation=\"softmax\")(x)# activation should be linear. <<<<<-------TASK\n",
    "outputs = layers.Dense(1)(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9918311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noori\\.conda\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "66/66 [==============================] - 15s 186ms/step - loss: 0.1946 - val_loss: 0.0320\n",
      "Epoch 2/100\n",
      "66/66 [==============================] - 11s 164ms/step - loss: 0.0633 - val_loss: 0.0319\n",
      "Epoch 3/100\n",
      "66/66 [==============================] - 12s 175ms/step - loss: 0.0382 - val_loss: 0.0333\n",
      "Epoch 4/100\n",
      "66/66 [==============================] - 12s 182ms/step - loss: 0.0365 - val_loss: 0.0340\n",
      "Epoch 5/100\n",
      "66/66 [==============================] - 12s 187ms/step - loss: 0.0355 - val_loss: 0.0363\n",
      "Epoch 6/100\n",
      "66/66 [==============================] - 12s 180ms/step - loss: 0.0348 - val_loss: 0.0339\n",
      "Epoch 7/100\n",
      "66/66 [==============================] - 12s 189ms/step - loss: 0.0343 - val_loss: 0.0344\n",
      "Epoch 8/100\n",
      "66/66 [==============================] - 12s 184ms/step - loss: 0.0341 - val_loss: 0.0326\n",
      "Epoch 9/100\n",
      "66/66 [==============================] - 12s 176ms/step - loss: 0.0340 - val_loss: 0.0320\n",
      "Epoch 10/100\n",
      "66/66 [==============================] - 12s 180ms/step - loss: 0.0350 - val_loss: 0.0323\n",
      "Epoch 11/100\n",
      "66/66 [==============================] - 11s 172ms/step - loss: 0.0339 - val_loss: 0.0329\n",
      "Epoch 12/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0337 - val_loss: 0.0321\n",
      "Epoch 13/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0333 - val_loss: 0.0323\n",
      "Epoch 14/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0330 - val_loss: 0.0327\n",
      "Epoch 15/100\n",
      "66/66 [==============================] - 11s 166ms/step - loss: 0.0327 - val_loss: 0.0322\n",
      "Epoch 16/100\n",
      "66/66 [==============================] - 11s 167ms/step - loss: 0.0327 - val_loss: 0.0323\n",
      "Epoch 17/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0325 - val_loss: 0.0324\n",
      "Epoch 18/100\n",
      "66/66 [==============================] - 12s 176ms/step - loss: 0.0325 - val_loss: 0.0332\n",
      "Epoch 19/100\n",
      "66/66 [==============================] - 12s 175ms/step - loss: 0.0323 - val_loss: 0.0321\n",
      "Epoch 20/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0322 - val_loss: 0.0324\n",
      "Epoch 21/100\n",
      "66/66 [==============================] - 12s 177ms/step - loss: 0.0322 - val_loss: 0.0327\n",
      "Epoch 22/100\n",
      "66/66 [==============================] - 11s 173ms/step - loss: 0.0319 - val_loss: 0.0319\n",
      "Epoch 23/100\n",
      "66/66 [==============================] - 12s 175ms/step - loss: 0.0319 - val_loss: 0.0324\n",
      "Epoch 24/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0317 - val_loss: 0.0323\n",
      "Epoch 25/100\n",
      "66/66 [==============================] - 11s 174ms/step - loss: 0.0319 - val_loss: 0.0320\n",
      "Epoch 26/100\n",
      "66/66 [==============================] - 12s 176ms/step - loss: 0.0318 - val_loss: 0.0319\n",
      "Epoch 27/100\n",
      "66/66 [==============================] - 11s 172ms/step - loss: 0.0317 - val_loss: 0.0321\n",
      "Epoch 28/100\n",
      "66/66 [==============================] - 11s 165ms/step - loss: 0.0317 - val_loss: 0.0319\n",
      "Epoch 29/100\n",
      "66/66 [==============================] - 11s 165ms/step - loss: 0.0316 - val_loss: 0.0318\n",
      "Epoch 30/100\n",
      "66/66 [==============================] - 11s 165ms/step - loss: 0.0317 - val_loss: 0.0319\n",
      "Epoch 31/100\n",
      "66/66 [==============================] - 11s 168ms/step - loss: 0.0316 - val_loss: 0.0318\n",
      "Epoch 32/100\n",
      "66/66 [==============================] - 11s 165ms/step - loss: 0.0315 - val_loss: 0.0318\n",
      "Epoch 33/100\n",
      "66/66 [==============================] - 11s 168ms/step - loss: 0.0316 - val_loss: 0.0318\n",
      "Epoch 34/100\n",
      "66/66 [==============================] - 11s 165ms/step - loss: 0.0315 - val_loss: 0.0319\n",
      "Epoch 35/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0315 - val_loss: 0.0318\n",
      "Epoch 36/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0315 - val_loss: 0.0318\n",
      "Epoch 37/100\n",
      "66/66 [==============================] - 12s 177ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 38/100\n",
      "66/66 [==============================] - 11s 167ms/step - loss: 0.0315 - val_loss: 0.0318\n",
      "Epoch 39/100\n",
      "66/66 [==============================] - 11s 168ms/step - loss: 0.0315 - val_loss: 0.0318\n",
      "Epoch 40/100\n",
      "66/66 [==============================] - 11s 168ms/step - loss: 0.0315 - val_loss: 0.0318\n",
      "Epoch 41/100\n",
      "66/66 [==============================] - 11s 168ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 42/100\n",
      "66/66 [==============================] - 11s 168ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 43/100\n",
      "66/66 [==============================] - 11s 168ms/step - loss: 0.0315 - val_loss: 0.0318\n",
      "Epoch 44/100\n",
      "66/66 [==============================] - 11s 168ms/step - loss: 0.0315 - val_loss: 0.0318\n",
      "Epoch 45/100\n",
      "66/66 [==============================] - 11s 171ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 46/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 47/100\n",
      "66/66 [==============================] - 11s 173ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 48/100\n",
      "66/66 [==============================] - 11s 168ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 49/100\n",
      "66/66 [==============================] - 11s 167ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 50/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 51/100\n",
      "66/66 [==============================] - 12s 178ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 52/100\n",
      "66/66 [==============================] - 12s 180ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 53/100\n",
      "66/66 [==============================] - 12s 176ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 54/100\n",
      "66/66 [==============================] - 12s 175ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 55/100\n",
      "66/66 [==============================] - 12s 175ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 56/100\n",
      "66/66 [==============================] - 12s 176ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 57/100\n",
      "66/66 [==============================] - 11s 174ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 58/100\n",
      "66/66 [==============================] - 11s 171ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 59/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 60/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 61/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 62/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 63/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 64/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 65/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 66/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0318\n",
      "Epoch 67/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 68/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 69/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 70/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 71/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 72/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 73/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 74/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 75/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 76/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 77/100\n",
      "66/66 [==============================] - 11s 172ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 78/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 79/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 80/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 81/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 82/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 83/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0314 - val_loss: 0.0318\n",
      "Epoch 84/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 85/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 86/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 87/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 88/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0318\n",
      "Epoch 89/100\n",
      "66/66 [==============================] - 11s 172ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 90/100\n",
      "66/66 [==============================] - 12s 178ms/step - loss: 0.0314 - val_loss: 0.0318\n",
      "Epoch 91/100\n",
      "66/66 [==============================] - 11s 174ms/step - loss: 0.0315 - val_loss: 0.0317\n",
      "Epoch 92/100\n",
      "66/66 [==============================] - 11s 169ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 93/100\n",
      "66/66 [==============================] - 11s 167ms/step - loss: 0.0315 - val_loss: 0.0318\n",
      "Epoch 94/100\n",
      "66/66 [==============================] - 11s 173ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 95/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 96/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 97/100\n",
      "66/66 [==============================] - 11s 170ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 98/100\n",
      "66/66 [==============================] - 12s 176ms/step - loss: 0.0314 - val_loss: 0.0317\n",
      "Epoch 99/100\n",
      "66/66 [==============================] - 11s 171ms/step - loss: 0.0314 - val_loss: 0.0318\n",
      "Epoch 100/100\n",
      "66/66 [==============================] - 11s 168ms/step - loss: 0.0314 - val_loss: 0.0317\n"
     ]
    }
   ],
   "source": [
    "# 8 - learning rate - determine the default value\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='mse')\n",
    "#model.compile(optimizer=Adam(lr=0.005), loss='mse')\n",
    "#This callback will stop the training when there is no improvement in the validation loss for 20 consecutive epochs. \n",
    "#regularization technique as preventing overfitting, model runtime gets shorter\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
    "\n",
    "\n",
    "# 6 - batch_size\n",
    "# 7 - epochs\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=256, epochs=100, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "531fceb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Loss Error')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting training loss and validation loss ----- \n",
    "plt.plot(history.history['loss'], 'o-',label='Training loss')\n",
    "plt.plot(history.history['val_loss'], 'o-',label='Validation loss')\n",
    "plt.legend()\n",
    "plt.title('Loss Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea8c1b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare test set to make predictions\n",
    "list_x_test=[]\n",
    "list_y_test=[]\n",
    "\n",
    "for engine_df in test_list:\n",
    "    #convert df to arr \n",
    "    engine_arr=engine_df.drop(columns=[\"engine_id\", \"cycle\"]).to_numpy()\n",
    "    X, y = split_sequences(engine_arr, seq_len)\n",
    "    \n",
    "    #use only last seq for each engine\n",
    "    list_x_test.append(X[-1].reshape((1, seq_len, num_features)))\n",
    "    list_y_test.append(y[-1].reshape((1, )))\n",
    "\n",
    "#concat alt alta  \n",
    "X_arr_test=np.concatenate(list_x_test)\n",
    "y_arr_test=np.concatenate(list_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a07e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to make inverse scaling for RUL column in order to produce proper prediction results\n",
    "#create a dummy df to take inverse only on one col --> \"y_pred\"\n",
    "def invTransform(scaler, y_pred, colNames):\n",
    "    dummy = pd.DataFrame(np.zeros((len(y_pred), len(colNames))), columns=colNames)\n",
    "    dummy[\"RUL\"] = y_pred\n",
    "    dummy = pd.DataFrame(scaler.inverse_transform(dummy), columns=colNames)\n",
    "    return dummy[\"RUL\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "540804d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction\n",
    "y_pred = model.predict(X_arr_test, verbose=0)\n",
    "\n",
    "#inverse scaling for y_pred values\n",
    "y_pred_inv=invTransform(scaler_1, y_pred, cols_normalize_1)\n",
    "\n",
    "y_pred_reshaped=y_pred_inv.reshape((len(y_pred_inv, )))\n",
    "y_pred_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "45fc86fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_truth=rul_1_gt[\"RUL_1gt\"].values\n",
    "y_truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4350e77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([112,  98,  69,  82,  91,  93,  91,  95, 111,  96,  97, 124,  95,\n",
       "       107,  83,  84,  50,  28,  87,  16,  57, 111, 113,  20, 145, 119,\n",
       "        66,  97,  90, 115,   8,  48, 106,   7,  11,  19,  21,  50, 142,\n",
       "        28,  18,  10,  59, 109, 114,  47, 135,  92,  21,  79, 114,  29,\n",
       "        26,  97, 137,  15, 103,  37, 114, 100,  21,  54,  72,  28, 128,\n",
       "        14,  77,   8, 121,  94, 118,  50, 131, 126, 113,  10,  34, 107,\n",
       "        63,  90,   8,   9, 137,  58, 118,  89, 116, 115, 136,  28,  38,\n",
       "        20,  85,  55, 128, 137,  82,  59, 117,  20], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b907d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([97.47088608, 97.47087532, 97.47086456, 97.47087532, 97.47087532,\n",
       "       97.47087532, 97.47088608, 97.47087532, 97.47087532, 97.47087532,\n",
       "       97.47087532, 97.47088608, 97.47087532, 97.47087532, 97.47088608,\n",
       "       97.47088608, 97.47087532, 97.47087532, 97.47088608, 97.47086456,\n",
       "       97.47088608, 97.47087532, 97.47088608, 97.47087532, 97.47088608,\n",
       "       97.47088608, 97.47087532, 97.47087532, 97.47087532, 97.47087532,\n",
       "       97.47085381, 97.47087532, 97.47087532, 97.47085381, 97.47085381,\n",
       "       97.47085381, 97.47087532, 97.47086456, 97.47089684, 97.47086456,\n",
       "       97.47086456, 97.47086456, 97.47087532, 97.47087532, 97.47087532,\n",
       "       97.47086456, 97.47087532, 97.47087532, 97.47086456, 97.47088608,\n",
       "       97.47087532, 97.47087532, 97.47087532, 97.47089684, 97.47088608,\n",
       "       97.47085381, 97.47087532, 97.47086456, 97.47088608, 97.47087532,\n",
       "       97.47086456, 97.47087532, 97.47086456, 97.47086456, 97.47089684,\n",
       "       97.47086456, 97.47088608, 97.47085381, 97.47087532, 97.47087532,\n",
       "       97.47088608, 97.47087532, 97.47088608, 97.47087532, 97.47088608,\n",
       "       97.47085381, 97.47086456, 97.47089684, 97.47087532, 97.47087532,\n",
       "       97.47085381, 97.47086456, 97.47088608, 97.47086456, 97.47087532,\n",
       "       97.47087532, 97.47088608, 97.47087532, 97.47087532, 97.47086456,\n",
       "       97.47085381, 97.47085381, 97.47086456, 97.47086456, 97.47088608,\n",
       "       97.47088608, 97.47088608, 97.47087532, 97.47089684, 97.47087532])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a43a0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error cal. for paper\n",
    "def score(y_true, y_pred, a1=10, a2=13):\n",
    "    score = 0\n",
    "    d = y_pred - y_true\n",
    "    for i in d:\n",
    "        if i >= 0 :\n",
    "            score += math.exp(i/a2) - 1   \n",
    "        else:\n",
    "            score += math.exp(- i/a1) - 1\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "083abc3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14748.593518098767"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(y_truth,y_pred_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "591e3181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_func(y_true, y_pred):\n",
    "    score_list = [\n",
    "                  round(score(y_true,y_pred), 2), \n",
    "                  round(mean_absolute_error(y_true,y_pred), 2),\n",
    "                  round(mean_squared_error(y_true,y_pred),2) ** 0.5,\n",
    "                  round(r2_score(y_true,y_pred), 2)\n",
    "                  ]\n",
    "    \n",
    "    \n",
    "    print(f' compatitive score: {score_list[0]}')\n",
    "    print(f' mean absolute error: {score_list[1]}')\n",
    "    print(f' root mean squared error: {score_list[2]}')\n",
    "    print(f' R2 score: {score_list[3]}')\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3abebfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " compatitive score: 14748.59\n",
      " mean absolute error: 37.26\n",
      " root mean squared error: 46.996914792356314\n",
      " R2 score: -0.28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_func(y_truth,y_pred_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948b40f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
