{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMAPSS_vanila_transformer_ARUL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Objective:\n",
        "The turbofan dataset features four datasets of increasing complexity. The engines operate normally in the beginning but develop a fault over time. For the training sets, the engines are run to failure, while in the test sets the time series end ‘sometime’ before failure. The goal is to predict the Remaining Useful Life (RUL) of each turbofan engine.\n",
        "\n",
        "The engine runs till a specified point in the test dataset. The goal is predicting the RUL at the last point of dataset.\n",
        "\n",
        "The datasets comprise simulations of several turbofan engines across time, with each row including the following information:\n",
        "1.   Engine unit number\n",
        "2.   Time, in cycles\n",
        "1.   Three operational settings\n",
        "2.   21 sensor readings"
      ],
      "metadata": {
        "id": "MEE3pv7c44ep"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mvDlbsU0pNA"
      },
      "source": [
        "# Loading the data: just upload the data trian_XXX.txt \n",
        "#to your Google Drive and update the path in line 6.\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1Lm4uNx00Xg"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import sklearn.metrics\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Dropout\n",
        "import tensorflow as tf\n",
        "import math\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uglACF0h00OM"
      },
      "source": [
        "# RUL_XXX.txt path\n",
        "rul_1_gt = pd.read_csv(\"/content/drive/My Drive/data/cmapss/RUL_FD001.txt\", header=None)\n",
        "\n",
        "rul_1_gt.rename(columns={0: \"RUL_1gt\"}, inplace=True)   \n",
        "\n",
        "column_name_dict={ 0: \"engine_id\", 1: \"cycle\", 2: \"op_set_1\", 3: \"op_set_2\", 4: \"op_set_3\", 5:\"sensor_1\", 6: \"sensor_2\",\n",
        "                   7: \"sensor_3\", 8: \"sensor_4\", 9: \"sensor_5\", 10: \"sensor_6\", 11: \"sensor_7\", 12: \"sensor_8\", 13: \"sensor_9\",\n",
        "                  14: \"sensor_10\", 15: \"sensor_11\", 16: \"sensor_12\", 17: \"sensor_13\", 18: \"sensor_14\", 19: \"sensor_15\", 20: \"sensor_16\",\n",
        "                  21: \"sensor_17\", 22: \"sensor_18\", 23: \"sensor_19\", 24: \"sensor_20\", 25: \"sensor_21\", 26: \"sensor_22\", 27: \"sensor_23\"}\n",
        "\n",
        "################   ################   ################   ################ \n",
        "# test_xxx.txt path\n",
        "test_1 = pd.read_csv(\"/content/drive/My Drive/data/cmapss/test_FD001.txt\", header=None, sep=' ')\n",
        "\n",
        "test_1.rename(columns=column_name_dict, inplace=True)\n",
        "\n",
        "################   ################   ################   ################ \n",
        "#train_xxx.txt path\n",
        "train_1 = pd.read_csv(\"/content/drive/My Drive/data/cmapss/train_FD001.txt\", header=None, sep=' ')\n",
        "\n",
        "train_1.rename(columns=column_name_dict, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tpQeFTg00GK"
      },
      "source": [
        "# Feture selection\n",
        "train_1.drop(\n",
        "    columns=[\"op_set_1\",\"op_set_2\",\"op_set_3\",\n",
        "             \"sensor_1\",\"sensor_5\",\"sensor_6\",\n",
        "             \"sensor_10\",\"sensor_16\",\"sensor_18\",\n",
        "             \"sensor_19\",\"sensor_22\",\"sensor_23\"], inplace=True)\n",
        "\n",
        "\n",
        "test_1.drop(\n",
        "    columns=[\"op_set_1\",\"op_set_2\",\"op_set_3\",\n",
        "             \"sensor_1\",\"sensor_5\",\"sensor_6\",\n",
        "             \"sensor_10\",\"sensor_16\",\"sensor_18\",\n",
        "             \"sensor_19\",\"sensor_22\",\"sensor_23\"], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_1"
      ],
      "metadata": {
        "id": "jHcJlq9g2pdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_1"
      ],
      "metadata": {
        "id": "2A-IbR-W2ta7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMwekuD600Ab"
      },
      "source": [
        "#Compute RUL values -- RUL for train_1 \n",
        "rul_1 = pd.DataFrame(train_1.groupby('engine_id')['cycle'].max()).reset_index()\n",
        "rul_1.columns = ['engine_id', 'max']\n",
        "\n",
        "rul_train_1 = train_1.merge(rul_1, on=['engine_id'], how='left')\n",
        "rul_train_1['RUL'] = rul_train_1['max'] - rul_train_1['cycle']\n",
        "rul_train_1.drop(['max'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rul_train_1"
      ],
      "metadata": {
        "id": "pEgGg0T92z-g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUIvSK9X0z5Q"
      },
      "source": [
        "#Compute RUL values -- RUL for test_1 \n",
        "rul_1_gt[\"engine_id\"]=rul_1_gt.index + 1\n",
        "\n",
        "max_1 = pd.DataFrame(test_1.groupby('engine_id')['cycle'].max()).reset_index()\n",
        "max_1.columns = ['engine_id', 'max']\n",
        "max_test_1 = test_1.merge(max_1, on=['engine_id'], how='left')\n",
        "rul_test_1 = max_test_1.merge(rul_1_gt, on=['engine_id'], how='left')\n",
        "\n",
        "rul_test_1['RUL'] = rul_test_1['max'] - rul_test_1['cycle'] + rul_test_1[\"RUL_1gt\"] \n",
        "rul_test_1.drop(['max', 'RUL_1gt'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rul_test_1"
      ],
      "metadata": {
        "id": "ECS738BV25dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_max_rul = rul_train_1[['engine_id', 'RUL']].groupby('engine_id').max().reset_index()\n",
        "df_max_rul['RUL'].hist(bins=15, figsize=(15,7))\n",
        "plt.xlabel('RUL')\n",
        "plt.ylabel('frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c1lteHNh5ZCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram reconfirms most engines break down around 200 cycles. Furthermore, the distribution is right skewed, with few engines lasting over 300 cycles."
      ],
      "metadata": {
        "id": "3IoIASNs5dZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#heatmap for correlation coefficient\n",
        "df_corr = rul_train_1.drop(columns=[\"engine_id\"]).corr()\n",
        "sns.set(font_scale=0.9)\n",
        "plt.figure(figsize=(24,16))\n",
        "sns.heatmap(df_corr, annot=True, fmt=\".4f\",vmin=-1, vmax=1, linewidths=.5, cmap = sns.color_palette(\"ch:s=.25,rot=-.25\", 200))\n",
        "\n",
        "plt.figtext(.6, 0.9,'correlation matrix of train_1', fontsize=30, ha='center')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6r77C80N5jT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psYK5eZ-0zyZ"
      },
      "source": [
        "# Creating piece-wise data frame from rul_train_1\n",
        "pw_train_1 = rul_train_1.copy()\n",
        "pw_train_1['RUL'] = np.where((pw_train_1.RUL > 125), 125, pw_train_1.RUL)\n",
        "pw_train_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8hqed910zq0"
      },
      "source": [
        "# Creating piece-wise data frame from rul_test_1\n",
        "pw_test_1 = rul_test_1.copy()\n",
        "pw_test_1['RUL'] = np.where((pw_test_1.RUL > 125), 125, pw_test_1.RUL)\n",
        "pw_test_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKTRAvEO0zjw"
      },
      "source": [
        "rul_train_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iw3QP9640zdm"
      },
      "source": [
        "rul_test_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K1_pKyA0zU7"
      },
      "source": [
        "# Just for plotting RUL\n",
        "pw = rul_train_1.sort_values(by=['RUL'])#, ascending=False\n",
        "pw['RUL'] = np.where((rul_train_1.RUL > 125), 125, rul_train_1.RUL)\n",
        "pw = pw.sort_values(by = 'RUL', ascending=False)\n",
        "\n",
        "# Plotting RUL\n",
        "ax = plt.gca()\n",
        "pw.plot(kind='line',x='engine_id',y='RUL',use_index=False, ax=ax)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBMcB38L1Whm"
      },
      "source": [
        "# max lifetime for each engine - Linear degredation function\n",
        "train_1.groupby('engine_id')['cycle'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP2_q2gP1WTX"
      },
      "source": [
        "# max lifetime for each engine - Picewise Linear degredation function\n",
        "pw_train_1.groupby('engine_id')['RUL'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQx0Ui561WIp"
      },
      "source": [
        "# train_1 minmax scaling\n",
        "cols_normalize_1 = pw_train_1.columns.difference(['engine_id','cycle'])\n",
        "\n",
        "scaler_1 = MinMaxScaler()\n",
        "norm_rul_train_1 = pd.DataFrame(scaler_1.fit_transform(pw_train_1[cols_normalize_1]), \n",
        "                                columns=cols_normalize_1, \n",
        "                                index=pw_train_1.index)\n",
        "\n",
        "norm_rul_train_1=pd.concat([norm_rul_train_1, pw_train_1[[\"engine_id\", \"cycle\"]]], axis=1)\n",
        "\n",
        "################   ################   ################   ################   ################\n",
        "\n",
        "# test_1 minmax scaling\n",
        "norm_rul_test_1 = pd.DataFrame(scaler_1.transform(pw_test_1[cols_normalize_1]), \n",
        "                                columns=cols_normalize_1, \n",
        "                                index=pw_test_1.index)\n",
        "\n",
        "norm_rul_test_1=pd.concat([norm_rul_test_1, pw_test_1[[\"engine_id\", \"cycle\"]]], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZHxqLbi1WAb"
      },
      "source": [
        "# Because we only have True RUL values for those records, we are only interested in the LAST CYCLE of each engine in the test set.\n",
        "# sort lists by the # of engines\n",
        "g_train_1=norm_rul_train_1.groupby('engine_id')\n",
        "g_test_1=norm_rul_test_1.groupby('engine_id')\n",
        "\n",
        "#list of dfs(engines)\n",
        "train_list = []\n",
        "test_list = []  \n",
        "\n",
        "for engineid in g_train_1.groups.keys():\n",
        "    train_list.append(g_train_1.get_group(engineid)) \n",
        "\n",
        "for engineid in g_test_1.groups.keys():\n",
        "    test_list.append(g_test_1.get_group(engineid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swZfgkZz1V5v"
      },
      "source": [
        "# creating sequences for each engine\n",
        "from numpy import array\n",
        "\n",
        "# df: df extracted from train_list, n_steps: window size\n",
        "def split_sequences(df, n_steps):\n",
        "    X, y = list(), list()\n",
        "    for i in range(len(df)):\n",
        "        end_ix = i + n_steps\n",
        "        if end_ix > len(df):\n",
        "            break\n",
        "        seq_x, seq_y = df[i:end_ix, 1:], df[end_ix-1, 0]\n",
        "        X.append(seq_x)\n",
        "        y.append(seq_y)\n",
        "    return array(X), array(y)\n",
        "\n",
        "\n",
        "list_x=[]\n",
        "list_y=[]\n",
        "for engine_df in train_list:\n",
        "    #convert df to arr \n",
        "    engine_arr=engine_df.drop(columns=[\"engine_id\", \"cycle\"]).to_numpy()\n",
        "    X, y = split_sequences(engine_arr, 21)#since smallest df has 21 rows\n",
        "    list_x.append(X)\n",
        "    list_y.append(y)\n",
        "\n",
        "X_arr_train=np.concatenate(list_x)\n",
        "y_arr_train=np.concatenate(list_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-t9pN6Q1VzW"
      },
      "source": [
        "X_arr_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szioSQ_J1Voh"
      },
      "source": [
        "y_arr_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02FHQ-FE1Vie"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads=8):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        if embed_dim % num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f\"embedding dimension = {embed_dim} should be divisible by number of heads = {num_heads}\"\n",
        "            )\n",
        "        self.projection_dim = embed_dim // num_heads\n",
        "        self.query_dense = layers.Dense(embed_dim)\n",
        "        self.key_dense = layers.Dense(embed_dim)\n",
        "        self.value_dense = layers.Dense(embed_dim)\n",
        "        self.combine_heads = layers.Dense(embed_dim)\n",
        "\n",
        "    def attention(self, query, key, value):\n",
        "        score = tf.matmul(query, key, transpose_b=True)\n",
        "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "        scaled_score = score / tf.math.sqrt(dim_key)\n",
        "        weights = tf.nn.softmax(scaled_score, axis=-1)\n",
        "        output = tf.matmul(weights, value)\n",
        "        return output, weights\n",
        "\n",
        "    def separate_heads(self, x, batch_size):\n",
        "        \"\"\"Reshape x to the shape (batch_size, -1, num_heads, embedding dimension.\n",
        "        Used to obtain the separate attention heads in each for each batch.\"\"\"\n",
        "        # -1 to ensure the dimension of the reshaped tensor is compatible with\n",
        "        # the original x\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim))\n",
        "        # ensure the transposed tensor dimensions are 0, 2, 1, 3\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # x.shape = [batch_size, seq_len, embedding_dim]\n",
        "        batch_size = tf.shape(inputs)[0]\n",
        "        query = self.query_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        key = self.key_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        value = self.value_dense(inputs)  # (batch_size, seq_len, embed_dim)\n",
        "        query = self.separate_heads(\n",
        "            query, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        key = self.separate_heads(\n",
        "            key, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        value = self.separate_heads(\n",
        "            value, batch_size\n",
        "        )  # (batch_size, num_heads, seq_len, projection_dim)\n",
        "        attention, weights = self.attention(query, key, value)\n",
        "        attention = tf.transpose(\n",
        "            attention, perm=[0, 2, 1, 3]\n",
        "        )  # (batch_size, seq_len, num_heads, projection_dim)\n",
        "        concat_attention = tf.reshape(\n",
        "            attention, (batch_size, -1, self.embed_dim)\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        # Recombine the heads\n",
        "        output = self.combine_heads(\n",
        "            concat_attention\n",
        "        )  # (batch_size, seq_len, embed_dim)\n",
        "        return output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ni4MyP5h1VcW"
      },
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        # Multi-head attention\n",
        "        attn_output = self.att(inputs)\n",
        "        # Apply dropout\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        # Add and norm\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        # Pass through FFNN, dropout again\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        # Final add and norm\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNJxcKgT1VWh"
      },
      "source": [
        "class TransformerBlockWithLeakyRelu(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlockWithLeakyRelu, self).__init__()\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"gelu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.leakyrelu = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"LeakyReLU\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        # Multi-head attention\n",
        "        attn_output = self.att(inputs)\n",
        "        # Apply dropout\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        # Apply leakyrelu after each sublayer\n",
        "        attn_output = self.leakyrelu(attn_output)\n",
        "        # Add and norm\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        # Pass through FFNN, dropout again\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        # Apply leakyrelu\n",
        "        ffn_output = self.leakyrelu(ffn_output)\n",
        "        # Final add and norm\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL-JVDpj1VRC"
      },
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    \"\"\"Add positional embedding\"\"\"\n",
        "    def __init__(self, maxlen, embed_dim):\n",
        "        super(PositionalEmbedding, self).__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = self.maxlen\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        return x + positions\n",
        "\n",
        "\n",
        "\n",
        "class LinearLayer(layers.Layer):\n",
        "    \"\"\"LinearLayer\"\"\"\n",
        "    def __init__(self, maxlen):\n",
        "        super(LinearLayer, self).__init__()\n",
        "        self.linear = layers.Dense(units=maxlen)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "      \n",
        "class Conv2D_layer(layers.Layer):\n",
        "    \"\"\"Convolutional Layer\"\"\"\n",
        "    def __init__(self, filters=3, kernel_size=2, activation=\"relu\"):\n",
        "        super(Conv2D_layer, self).__init__()\n",
        "        self.Conv2D = layers.Conv2D(filters=filters, kernel_size=kernel_size, activation=activation)\n",
        "    def call(self, x):\n",
        "        x = self.Conv2D(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkybwadY1VLp"
      },
      "source": [
        "seq_len = X_arr_train.shape[1]  \n",
        "num_features = X_arr_train.shape[2]\n",
        "\n",
        "\n",
        "embed_dim = 512  # Embedding size\n",
        "num_heads = 2 # 1 - Number of attention heads\n",
        "ff_dim = 512 # 3 - feed forward network hidden dimension - Hidden layer size in feed forward network inside transformer\n",
        "droupout_rate = 0.3  # 4 - Droupoutrate\n",
        "\n",
        "inputs = layers.Input(shape=(seq_len, num_features))\n",
        "embedding_layer = LinearLayer(embed_dim)\n",
        "positional_embeddings = PositionalEmbedding(maxlen=seq_len, embed_dim=embed_dim)\n",
        "\n",
        "\n",
        "# 3. Building the network:\n",
        "x = embedding_layer(inputs) # \"Embedding dimension is the sequence length\"\n",
        "x = positional_embeddings(x) # adding positional embeddings\n",
        "\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim, droupout_rate)\n",
        "transformer_block2 = TransformerBlock(embed_dim, num_heads, ff_dim, droupout_rate)\n",
        "\n",
        "# 2 - Number of Transformer_blocks\n",
        "x = transformer_block(x)\n",
        "x = transformer_block2(x)\n",
        "\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(droupout_rate)(x)\n",
        "\n",
        "# 5 - Activation parameter\n",
        "x = layers.Dense(64, activation=\"sigmoid\")(x)\n",
        "x = layers.Dropout(droupout_rate)(x)\n",
        "x = layers.Dense(32, activation=\"sigmoid\")(x)\n",
        "x = layers.Dropout(droupout_rate)(x)\n",
        "\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XU1HvmQR1VE_"
      },
      "source": [
        "# root mean squared error (rmse) for regression (only for Keras tensors)\n",
        "def rmse(y_true, y_pred):\n",
        "    from keras import backend\n",
        "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
        "\n",
        "# 6 - learning rate - determine the default value\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss=rmse)\n",
        "\n",
        "#Callback in the validation loss for 20 consecutive epochs. \n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
        "\n",
        "# 7 - batch_size\n",
        "# 8 - epochs\n",
        "history = model.fit(\n",
        "    X_arr_train, y_arr_train, batch_size=128, epochs=50, callbacks=[callback], validation_split=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lxTrQVZ1VAR"
      },
      "source": [
        "# plotting training loss and validation loss\n",
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(10, 6), dpi=100)\n",
        "\n",
        "plt.plot(history.history['loss'], 'o-',label='Training loss')\n",
        "plt.plot(history.history['val_loss'], 'o-',label='Validation loss')\n",
        "plt.legend()\n",
        "plt.title('Loss Error')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('RMSE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69tMp9uk1z6y"
      },
      "source": [
        "#prepare Test set to make predictions\n",
        "list_x_test=[]\n",
        "list_y_test=[]\n",
        "\n",
        "for engine_df in test_list:\n",
        "    #convert df to arr \n",
        "    engine_arr=engine_df.drop(columns=[\"engine_id\", \"cycle\"]).to_numpy()\n",
        "    X, y = split_sequences(engine_arr, seq_len)\n",
        "    list_x_test.append(X[-1].reshape((1, seq_len, num_features)))\n",
        "    list_y_test.append(y[-1].reshape((1, )))  \n",
        "Xx_arr_test=np.concatenate(list_x_test)\n",
        "yy_arr_test=np.concatenate(list_y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdOIWXWi1zwI"
      },
      "source": [
        "Xx_arr_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pziF9cT71zkT"
      },
      "source": [
        "yy_arr_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igaC04rg1zab"
      },
      "source": [
        "# inverse scaling for RUL column\n",
        "#dummy to take inverse only on one col --> \"y_pred\"\n",
        "def invTransform(scaler, y_pred, colNames):\n",
        "    dummy = pd.DataFrame(np.zeros((len(y_pred), len(colNames))), columns=colNames)\n",
        "    dummy[\"RUL\"] = y_pred\n",
        "    dummy = pd.DataFrame(scaler.inverse_transform(dummy), columns=colNames)\n",
        "    return dummy[\"RUL\"].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDFd6BWl1zP0"
      },
      "source": [
        "# Prediction on Test set\n",
        "y_test_pred = model.predict(Xx_arr_test, verbose=0)\n",
        "\n",
        "# Inverse scaling for y_pred values\n",
        "y_test_pred_inv=invTransform(scaler_1, y_test_pred, cols_normalize_1)\n",
        "\n",
        "y_test_pred_reshaped=y_test_pred_inv.reshape((len(y_test_pred_inv, )))\n",
        "y_test_pred_reshaped.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syl_a5j71zHo"
      },
      "source": [
        "y_truth=rul_1_gt[\"RUL_1gt\"].values\n",
        "y_truth.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted = \\n\", np.round(y_test_pred_reshaped, 2), \"\\n\\n Actual = \\n\", y_truth)  "
      ],
      "metadata": {
        "id": "e5LL5jW7Cs1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGJNIKt11y_v"
      },
      "source": [
        "import math\n",
        "y_actual = y_truth\n",
        "y_predicted = y_test_pred_reshaped\n",
        " \n",
        "MSE = np.square(np.subtract(y_actual,y_predicted)).mean() \n",
        " \n",
        "RMSE = math.sqrt(MSE)\n",
        "\n",
        "print(\"Mean Square Error: \", MSE)\n",
        "print(\"Root Mean Square Error: \",RMSE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjsjrleP1y5A"
      },
      "source": [
        "def score_func(y_true, y_pred):\n",
        "    score_list = [\n",
        "                  round(score(y_true,y_pred), 2), \n",
        "                  round(mean_absolute_error(y_true,y_pred), 2),\n",
        "                  round(mean_squared_error(y_true,y_pred), 2) ** 0.5,\n",
        "                  round(r2_score(y_true,y_pred), 2)\n",
        "                  ]\n",
        "\n",
        "    print(f' compatitive score: {score_list[0]}')\n",
        "    print(f' mean absolute error: {score_list[1]}')\n",
        "    print(f' root mean squared error: {score_list[2]}')\n",
        "    print(f' R2 score: {score_list[3]}')\n",
        "    \n",
        "    return "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4UlCzL51yxo"
      },
      "source": [
        "score_func(y_truth, y_test_pred_reshaped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BRc5U-P1yqu"
      },
      "source": [
        "def score(y_true, y_pred, a1=10, a2=13):\n",
        "    score = 0\n",
        "    d = y_pred - y_true\n",
        "    for i in d:\n",
        "        if i >= 0 :\n",
        "            score += math.exp(i/a2) - 1   \n",
        "        else:\n",
        "            score += math.exp(- i/a1) - 1\n",
        "    return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ2EOprF1yiq"
      },
      "source": [
        "score(y_truth,y_test_pred_reshaped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjCgremP0zBr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}